5.1.1 基线模型
=============================================
It might seem silly to say this, but the simplest possible way to use the information we have as inputs is to ignore the inputs entirely and to predict future outcomes based only on the mean value of the output we��ve seen in the past. In the example of an actuary, we could completely ignore a person��s health records and simply guess that they��ll live as long as the average person lives.

Guessing the mean outcome for every case isn��t as naive as it might seem: if we��re interested in making predictions that are as close to the truth as possible without using any other information, guessing the mean output turns out to be the best guess we can
possibly make.

>A little bit of work has to go into defining ��best�� to give this claim a definite meaning. If you��re uncomfortable with us throwing around the word ��best�� without saying what we mean, we promise that we��ll give a formal definition soon.

Before we discuss how to make the best possible guess, let��s suppose that we have data from an imaginary actuarial database, a portion of which is shown in Table 5-1.

table5.1

Because this is a totally new data set, it��s a good idea to follow our suggestions in [Chapter 2][1] and explore the data a bit before doing any formal analysis. We have one numeric column and another column that��s a dummy-coded factor, and so it��s natural to make density plots to compare smokers with nonsmokers (see the resulting plot in Figure 5-1):

    ages <- read.csv('data/longevity.csv')
    
    ggplot(ages, aes(x = AgeAtDeath, fill = factor(Smokes))) +
    geom_density() +
    facet_grid(Smokes ~ .)

figure5.1

The resulting plot makes it seem reasonable to believe that smoking matters for lon-gevity because the center of the nonsmokers�� life span distribution is shifted to the right of the center of the smokers�� life spans. In other words, the average life span of a non-smoker is longer than the average life span of a smoker. But before we describe how you can use the information we have about a person��s smoking habits to make predic-tions about her longevity, let��s pretend for a minute that you didn��t have any of this information. In that case, you��d need to pick a single number as your prediction for every new person, regardless of her smoking habits. So what number should you pick?

That question isn��t a trivial one, because the number you should pick depends on what you think it means to make good predictions. There are a lot of reasonable ways to define the accuracy of predictions, but there is a single measure of quality that��s been dominant for virtually the entire history of statistics. This measure is called squared error. If you��re trying to predict a value y (the true output) and you guess h (your hy-pothesis about y), then the squared error of your guess is simply (y - h) ^ 2.

Beyond the value inherent in following conventions that others will understand, there are a lot of good reasons for why you might want to use squared error to measure the quality of your guesses. We won��t get into them now, but we will talk a little bit more about the ways one can measure error in [Chapter 7][2], when we talk about optimization algorithms in machine learning. For now, we��ll just try to convince you of something fundamental: if we��re using squared error to measure the quality of our predictions, then the best guess we can possibly make about a person��s life span��without any additional information about a person��s habits��is the average person��s longevity.

To convince you of this claim, let��s see what happens if we use other guesses instead of the mean. With the longevity data set, the mean AgeAtDeath is 72.723, which we��ll round up to 73 for the time being. We can then ask: ��How badly would we have pre-dicted the ages of the people in our data set if we��d guessed that they all lived to be 73?��

To answer that question in R, we can use the following code, which combines all of the squared errors for each person in our data set by computing the mean of the squared errors, which we��ll call the mean squared error (or MSE for short):

    ages <- read.csv('data/longevity.csv')
    
    guess <- 73
    
    with(ages, mean((AgeAtDeath - guess) ^ 2))
    #[1] 32.991

After running this code, we find that the mean squared error we make by guessing 73 for every person in our data set is 32.991. But that, by itself, shouldn��t be enough to convince you that we��d do worse with a guess that��s not 73. To convince you that 73 is the best guess, we need to consider how we��d perform if we made some of the other possible guesses. To compute those other values in R, we loop over a sequence of possible guesses ranging from 63 to 83:

    ages <- read.csv('data/longevity.csv')
    
    guess.accuracy <- data.frame()
    
    for (guess in seq(63, 83, by = 1))
    {
        prediction.error <- with(ages, 
                                 mean((AgeAtDeath - guess) ^ 2))
        guess.accuracy <- rbind(guess.accuracy, 
                                data.frame(Guess = guess,
                                           Error = prediction.error))
    }

    ggplot(guess.accuracy, aes(x = Guess, y = Error)) +
        geom_point() +
        geom_line()


figure5-2

As you can see from looking at Figure 5-2, using any guess other than 73 gives us worse predictions for our data set. This is actually a general theoretical result that can be proven mathematically: to minimize squared error, you want to predict the mean value in your data set. One important implication of this is that the predictive value of having information about smoking should be measured in terms of the amount of improve-ment you get from using this information over just using the mean value as your guess for every person you see.




[1]: ./2.1.md  "Chapter 2"
[2]: ./7.1.md  "Chapter 7"